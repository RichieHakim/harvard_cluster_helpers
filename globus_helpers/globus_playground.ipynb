{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import warnings\n",
    "import copy\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import bnpm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Strategy\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# Log in (if not already logged in)\n",
    "globus login\n",
    "\n",
    "# Start transfer\n",
    "globus transfer -r <SOURCE_ENDPOINT_ID>:\"/path/to/source_folder\" <DESTINATION_ENDPOINT_ID>:\"/path/to/destination_folder\" --label \"HMS-RC to FAS RC Holyoke Transfer\"\n",
    "\n",
    "# Optionally, display tasks\n",
    "globus task list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare globus CLI\n",
    "- activate an environment and install globus with pip: `pip install globus-cli`\n",
    "- log in with `globus login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Your Endpoints\n",
    "\n",
    "You need the unique endpoint IDs for both storage systems. You can search for them in CLI using:\n",
    "```bash\n",
    "globus endpoint search \"HMS-RC\"\n",
    "globus endpoint search \"FAS RC Holyoke\"\n",
    "```\n",
    "Review the search results and note the endpoint IDs (they typically look like a long UUID). If you already have these IDs from your institutionâ€™s documentation, you can use them directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your endpoints\n",
    "Note that when you go to do an action, you'll need to authenticate your access to an endpoint. Just follow instructions. You should probably be required to authenticate via a call like the following:\n",
    "`globus session consent 'urn:globus:auth:scope:transfer.api.globus.org:all[*https://auth.globus.org/scopes/{endpoint}/data_access]'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"HMS\": {\n",
    "        \"endpoint\": \"b0718922-7031-11e9-b7f8-0a37f382de32\",\n",
    "        \"hostname\": \"transfer.rc.hms.harvard.edu\",\n",
    "        \"username\": \"rh183\",\n",
    "    },\n",
    "    \"FAS\": {\n",
    "        \"endpoint\": \"1156ed9e-6984-11ea-af52-0201714f6eab\",\n",
    "        \"hostname\": \"rc.fas.harvard.edu\",\n",
    "        \"username\": \"rhakim\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_CLI(\"globus session consent 'urn:globus:auth:scope:transfer.api.globus.org:all[*https://auth.globus.org/scopes/b0718922-7031-11e9-b7f8-0a37f382de32/data_access]'\")\n",
    "# call_CLI(\"globus session consent 'urn:globus:auth:scope:transfer.api.globus.org:all[*https://auth.globus.org/scopes/1156ed9e-6984-11ea-af52-0201714f6eab/data_access]'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPERS\n",
    "\n",
    "def call_CLI(command):\n",
    "    ## Call the CLI command, collect and print the outputs\n",
    "    print(command)\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "    print(output.decode('utf-8'))\n",
    "    print(error.decode('utf-8'))\n",
    "    return output.decode('utf-8')\n",
    "\n",
    "def ls(endpoint, path):\n",
    "    ## List the files in the given path\n",
    "    return call_CLI(f'globus ls {endpoint}:\"{path}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize ssh_transfer\n",
    "def make_ssh_sftp(\n",
    "    hostname,\n",
    "    username,\n",
    "):\n",
    "    import getpass\n",
    "    print(f\"Connecting to {hostname} as {username}\")\n",
    "    if hostname == 'rc.fas.harvard.edu':\n",
    "        ssh_t = bnpm.server.ssh_interface()\n",
    "        ssh_t.fasrc_connect(username=username)\n",
    "    elif hostname == 'transfer.rc.hms.harvard.edu':\n",
    "        use_localSshKey = False\n",
    "        pw = bnpm.server.pw_encode(getpass.getpass(prompt='Password for HMS: ')) if (use_localSshKey==False) else None\n",
    "        # path_sshKey = '/home/rich/.ssh/id_rsa' if use_localSshKey else None\n",
    "        \n",
    "        ssh_t = bnpm.server.ssh_interface(\n",
    "            nbytes_toReceive=20000,\n",
    "            recv_timeout=1,\n",
    "            verbose=True,\n",
    "        )\n",
    "        ssh_t.o2_connect(\n",
    "            hostname=hostname,\n",
    "            username=username,\n",
    "            password=bnpm.server.pw_decode(pw),\n",
    "            # key_filename=path_sshKey,\n",
    "            look_for_keys=False,\n",
    "            passcode_method=1,\n",
    "            verbose=0,\n",
    "            skip_passcode=False,    \n",
    "        )\n",
    "    else:\n",
    "        warnings.warn(f\"Unknown hostname: {hostname}\")\n",
    "    \n",
    "    sftp_t = bnpm.server.sftp_interface(ssh_client=ssh_t.client)\n",
    "        \n",
    "    return {\n",
    "        \"ssh\": ssh_t,\n",
    "        \"sftp\": sftp_t,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dict called 'conn' that holds sftp connections for the different servers in params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = {name: make_ssh_sftp(hostname=params[name][\"hostname\"], username=params[name][\"username\"]) for name in [\"FAS\", \"HMS\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find paths to transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_found_1 = conn['HMS']['sftp'].search_recursive(\n",
    "    dir_outer='/n/files/Neurobio/MICROSCOPE/Rich/data/2pRAM/facerhythm_stroke_biomarker_exp/camera/',\n",
    "    # dir_outer='/n/holylabs/LABS/bsabatini_lab/Users/rhakim/data/2pRAM/facerhythm_stroke_biomarker_exp/camera/20250303/PS46',\n",
    "    # reMatch='video1.*avi',\n",
    "    reMatch='video1.*avi|timestamp1.*csv',\n",
    "    # reMatch='PointTracker\\.h5',\n",
    "    # reMatch='PS46',\n",
    "    # reMatch_in_path='20250319.*PS46',\n",
    "    reMatch_in_path='PS47',\n",
    "    depth=5,\n",
    "    find_folders=False,\n",
    "    find_files=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get size of transfer\n",
    "sizes_source = {}\n",
    "for path in paths_found_1:\n",
    "    if conn['HMS']['sftp'].isdir_remote(path):\n",
    "        props = conn['HMS']['sftp'].list_fileSizes_recursive(path)\n",
    "        sizes_source[path] = sum([s for p, s in props.items()])\n",
    "    else:\n",
    "        props = conn['HMS']['sftp'].get_fileProperties(path)\n",
    "        sizes_source[path] = props['size']\n",
    "        \n",
    "size_total = sum(sizes_source.values())\n",
    "print(f\"Total size: {size_total/1e9:.2f} GB\")\n",
    "print(\"\\n\")\n",
    "print(f\"Sizes of individual elements:\")\n",
    "for path, size in sizes_source.items():\n",
    "    print(f\"{size/1e9:.2f} GB: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_source = copy.deepcopy(paths_found_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_found_2 = conn['FAS']['sftp'].search_recursive(\n",
    "    # dir_outer='/n/files/Neurobio/MICROSCOPE/Rich/data/2pRAM/facerhythm_stroke_biomarker_exp/camera/',\n",
    "    # dir_outer='/n/holylabs/LABS/bsabatini_lab/Users/rhakim/data/2pRAM/facerhythm_stroke_biomarker_exp/camera/',\n",
    "    # dir_outer='/n/netscratch/bsabatini_lab/Lab/rhakim/data/2pRAM/facerhythm_stroke_biomarker_exp/camera/',\n",
    "    # dir_outer='/n/netscratch/bsabatini_lab/Lab/rhakim/',\n",
    "    dir_outer='/n/netscratch/bsabatini_lab/Lab/rhakim/data/2pRAM/facerhythm_stroke_biomarker_exp/camera/cam1',\n",
    "    # reMatch='video1.*avi',\n",
    "    # reMatch='PS46',\n",
    "    reMatch='video1.*avi|timestamp1.*csv',\n",
    "    reMatch_in_path='',\n",
    "    depth=8,\n",
    "    find_folders=False,\n",
    "    find_files=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_found_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_dest = \"/n/netscratch/bsabatini_lab/Lab/rhakim/data/2pRAM/facerhythm_stroke_biomarker_exp/camera/cam1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths_source_dest = {source: str(Path(paths_dest) / Path(source).parts[-1] / Path(source).parts[-2]) for source in paths_source}\n",
    "# paths_source_dest = {source: str(Path(paths_dest) / Path(source).parts[-2] / Path(source).parts[-3] / Path(source).parts[-1]) for source in paths_source}\n",
    "paths_source_dest = {source: str(Path(paths_dest) / Path(source).parts[-2] / Path(source).parts[-3] / Path(source).parts[-1]) for source in paths_source if not ('PS46' in source) and not ('L_' in Path(source).parts[-2]) and not ('calibration' in Path(source).parts[-2])}\n",
    "paths_source_dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if the destination exists, and if so check if the size is the same\n",
    "paths_toTransfer = {}\n",
    "for source, dest in paths_source_dest.items():\n",
    "    if conn['FAS']['sftp'].isdir_remote(dest):\n",
    "        props = conn['FAS']['sftp'].list_fileSizes_recursive(dest)\n",
    "        size_dest = sum([s for p, s in props.items()])\n",
    "    else:\n",
    "        props = conn['FAS']['sftp'].get_fileProperties(dest)\n",
    "        if props is None:\n",
    "            # print(f\"File does not exist. {source} -> {dest}\")\n",
    "            paths_toTransfer[source] = dest\n",
    "            continue\n",
    "        size_dest = props['size']\n",
    "        \n",
    "    size_source = sizes_source[source]\n",
    "    \n",
    "    if size_source == size_dest:\n",
    "        # print(f\"File already exists and is the same size. {source} -> {dest}\")\n",
    "        pass\n",
    "    elif size_source > size_dest:\n",
    "        print(f\"File exists but is smaller. {source} -> {dest}\")\n",
    "        paths_toTransfer[source] = dest\n",
    "    elif size_source < size_dest:\n",
    "        print(f\"WARNING!!!!! File exists but is larger. {source} -> {dest}\")\n",
    "    else:\n",
    "        print(f\"Something went wrong. {source} -> {dest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, dest in paths_toTransfer.items():\n",
    "    print(f\"{source} -> {dest}\")\n",
    "print(f\"Number of files to transfer: {len(paths_toTransfer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globus transfer -r <SOURCE_ENDPOINT_ID>:\"/path/to/source_folder\" <DESTINATION_ENDPOINT_ID>:\"/path/to/destination_folder\" --label \"HMS-RC to FAS RC Holyoke Transfer\"\n",
    "\n",
    "for ii, (source, dest) in enumerate(paths_toTransfer.items()):\n",
    "    ## if a folder\n",
    "    if conn['HMS']['sftp'].isdir_remote(source):\n",
    "        print(f\"Initiating transfer:\\nContents from: {source}\\nInto:          {dest}\")\n",
    "        flag_folder = \"-r\"\n",
    "    else:\n",
    "        print(f\"Initiating transfer:\\From: {source}\\nTo: {dest}\")\n",
    "        flag_folder = \"\"\n",
    "    \n",
    "    command = f'globus transfer {flag_folder} {params[\"HMS\"][\"endpoint\"]}:\"{source}\" {params[\"FAS\"][\"endpoint\"]}:\"{dest}\" --label \"HMS-RC to FAS RC Holyoke Transfer_{ii}\"'\n",
    "    # print(command)\n",
    "    call_CLI(command)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = conn['FAS']['sftp'].search_recursive(\n",
    "    # dir_outer='/n/files/Neurobio/MICROSCOPE/Rich/data/2pRAM/facerhythm_stroke_biomarker_exp/camera/',\n",
    "    # dir_outer='/n/holylabs/LABS/bsabatini_lab/Users/rhakim/data/2pRAM/facerhythm_stroke_biomarker_exp/camera/',\n",
    "    # dir_outer='/n/netscratch/bsabatini_lab/Lab/rhakim/data/2pRAM/facerhythm_stroke_biomarker_exp/camera/',\n",
    "    dir_outer='/n/holylabs/LABS/bsabatini_lab/Users/rhakim/analysis/face_rhythm/PS46/run_20250325',\n",
    "    # dir_outer='/n/holylabs/LABS/bsabatini_lab/Users/rhakim/analysis/face_rhythm/PS46/run_20250325',\n",
    "    # reMatch='video1.*avi',\n",
    "    # reMatch='PS46',\n",
    "    reMatch='PointTracker\\.h5',\n",
    "    # reMatch_in_path='',\n",
    "    depth=14,\n",
    "    find_folders=False,\n",
    "    find_files=True,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check status with CLI: `globus task list`\n",
    "## Show only tasks that are active\n",
    "\n",
    "call_CLI('globus task list --filter-status ACTIVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check status with CLI: `globus task list`\n",
    "## Show only tasks that are active\n",
    "\n",
    "call_CLI('globus task list --filter-status FAILED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stop a specific task\n",
    "# call_CLI('globus task cancel <task_id>')\n",
    "call_CLI('globus task cancel 8c5f04ff-04e4-11f0-b64c-0e283342ad7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stop all active jobs\n",
    "call_CLI('globus task cancel --all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
